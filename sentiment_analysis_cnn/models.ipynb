{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pyplot \u001B[38;5;28;01mas\u001B[39;00m plt\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tokenizer\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequence\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pad_sequences\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m to_categorical\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\__init__.py:45\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2 \u001B[38;5;28;01mas\u001B[39;00m _tf2\n\u001B[0;32m     43\u001B[0m _tf2\u001B[38;5;241m.\u001B[39menable()\n\u001B[1;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __internal__\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __operators__\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m audio\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:13\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m eager_context\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m feature_column\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m function\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m graph_util\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\feature_column\\__init__.py:8\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.feature_column namespace\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_column\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_column_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DenseColumn \u001B[38;5;66;03m# line: 1777\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_column\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_column_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FeatureTransformationCache \u001B[38;5;66;03m# line: 1962\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_column\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_column_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SequenceDenseColumn \u001B[38;5;66;03m# line: 1941\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:38\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m readers\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[1;32m---> 38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_column\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m feature_column \u001B[38;5;28;01mas\u001B[39;00m fc_old\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_column\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m feature_column_v2_types \u001B[38;5;28;01mas\u001B[39;00m fc_types\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_column\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialization\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:41\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sparse_tensor \u001B[38;5;28;01mas\u001B[39;00m sparse_tensor_lib\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_shape\n\u001B[1;32m---> 41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m base\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m array_ops\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m array_ops_stack\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\python\\layers\\base.py:16\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# =============================================================================\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy_tf_layers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m base\n\u001B[0;32m     18\u001B[0m InputSpec \u001B[38;5;241m=\u001B[39m base\u001B[38;5;241m.\u001B[39mInputSpec\n\u001B[0;32m     20\u001B[0m keras_style_scope \u001B[38;5;241m=\u001B[39m base\u001B[38;5;241m.\u001B[39mkeras_style_scope\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\python\\keras\\__init__.py:25\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minput_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\python\\keras\\models.py:20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m metrics \u001B[38;5;28;01mas\u001B[39;00m metrics_module\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m optimizer_v1\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:35\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_conversion\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_shape\n\u001B[1;32m---> 35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m base_layer\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\python\\keras\\activations.py:18\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Built-in activation functions.\"\"\"\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m advanced_activations\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeneric_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deserialize_keras_object\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeneric_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialize_keras_object\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py:22\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Generic layers.\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# pylint: disable=g-bad-import-order\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# pylint: disable=g-import-not-at-top\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minput_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minput_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InputLayer\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minput_spec\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InputSpec\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py:24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distributed_training_utils\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m base_layer\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras_tensor\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m node \u001B[38;5;28;01mas\u001B[39;00m node_module\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:55\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmixed_precision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m autocast_variable\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmixed_precision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m loss_scale_optimizer\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmixed_precision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m policy\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaved_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layer_serialization\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m generic_utils\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1331\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:935\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:990\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1123\u001B[0m, in \u001B[0;36mget_code\u001B[1;34m(self, fullname)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:754\u001B[0m, in \u001B[0;36m_compile_bytecode\u001B[1;34m(data, name, bytecode_path, source_path)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:491\u001B[0m, in \u001B[0;36m_verbose_message\u001B[1;34m(message, verbosity, *args)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-17T14:01:56.634316600Z",
     "start_time": "2025-01-17T14:01:52.781035400Z"
    }
   },
   "id": "fd7b2eb04cb907cf",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.manual_seed(2023)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.634316600Z"
    }
   },
   "id": "84b0528a7886bd1f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\connor\\PycharmProjects\\portfolio\\sentiment_analysis_cnn\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\connor\\PycharmProjects\\portfolio\\sentiment_analysis_cnn\\test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-17T14:01:56.637317600Z",
     "start_time": "2025-01-17T14:01:56.635319500Z"
    }
   },
   "id": "4e039988147f8d17",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(train.describe())\n",
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.635319500Z"
    }
   },
   "id": "74c49887c531ba05",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "grouped_count = train.groupby(train['sentiment']).count()\n",
    "\n",
    "grouped_count.plot(kind='bar', ylabel='Count', xlabel='Sentiment', title='Sentiment Distribution', legend=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.636316300Z"
    }
   },
   "id": "edb90479d51f1d3e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(603/len(train))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.637317600Z"
    }
   },
   "id": "9374f1ca83f74305",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = train.text\n",
    "y_train = train.sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-17T14:01:56.739831300Z",
     "start_time": "2025-01-17T14:01:56.638318300Z"
    }
   },
   "id": "b7419645d894da62",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.639317700Z"
    }
   },
   "id": "4d93aa25ca46a995",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Synthetic Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d071b78866c2da15"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Synthetic Data Creation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "np.random.seed(2023)\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def synonym_replacement(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    new_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        synsets = wordnet.synsets(word, pos=wordnet_tag(tag))\n",
    "        if synsets:\n",
    "            synonyms = [lemma.name() for synset in synsets for lemma in synset.lemmas()]\n",
    "            synonym = np.random.choice(synonyms) if synonyms else word\n",
    "            new_words.append(synonym)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def wordnet_tag(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "for itr, row in enumerate(X_train):\n",
    "    print(row)\n",
    "    print(synonym_replacement(row))\n",
    "    break\n",
    "# Add synonym replaced sentences to X_train and update y_train\n",
    "X_train_synth = []\n",
    "y_train_synth = []\n",
    "\n",
    "for itr, row in enumerate(X_train):\n",
    "    X_train_synth.append(row)\n",
    "    # Add synthetic row\n",
    "    X_train_synth.append(synonym_replacement(row))\n",
    "    # append the true y-value twice. Once for the real row, and once for the same synth row\n",
    "    y_train_synth.append(y_train[itr])\n",
    "    y_train_synth.append(y_train[itr])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.639317700Z"
    }
   },
   "id": "900a4418210653d5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(X_train[0:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.640318400Z"
    }
   },
   "id": "f2c304866f2e13e1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(pd.DataFrame(X_train_synth)[0:20])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.641317100Z"
    }
   },
   "id": "e3d4f1c84c4cc63a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shuffle and split into train/valid\n",
    "\n",
    "Test set is on Kaggle and only takes id and labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63967c88d053174d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.tensor(y_train_synth).unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.642316600Z"
    }
   },
   "id": "f0efb5a3230d170",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train_synth, y_train_synth, shuffle=True, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.643318800Z"
    }
   },
   "id": "ffc71922c056d0c4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test).describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.643318800Z"
    }
   },
   "id": "3f510e9adf19bbff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# to_categorical has to take values 0-n not 1-n\n",
    "y_train = [x-1 for x in y_train]\n",
    "y_test = [x-1 for x in y_test]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.644318700Z"
    }
   },
   "id": "f26ca7800233ec63",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert categories to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=5)\n",
    "y_test = to_categorical(y_test, num_classes=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.645318500Z"
    }
   },
   "id": "7e9837b1327f7f4f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test).describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.645318500Z"
    }
   },
   "id": "31ab696a3fb90880",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bag of Words feature_extractor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94f7f8020781338b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train_gram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_gram_vectorizer.fit(X_train)\n",
    "X_train_gram_vectorizer = X_train_gram_vectorizer.transform(X_train).toarray()\n",
    "X_train_bag_of_words = torch.tensor(X_train_gram_vectorizer)\n",
    "\n",
    "X_test_gram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_test_gram_vectorizer.fit(X_test)\n",
    "X_test_gram_vectorizer = X_test_gram_vectorizer.transform(X_train).toarray()\n",
    "X_test_bag_of_words = torch.tensor(X_test_gram_vectorizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.646317400Z"
    }
   },
   "id": "4cd0336a2f3b185d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train_bag_of_words.unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.647317600Z"
    }
   },
   "id": "fbd6797962580aee",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0da8a1490ccdfea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# sequence encode\n",
    "encoded_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# pad sequences\n",
    "# max_length will be a tunable hyperparameter\n",
    "max_length = 200\n",
    "X_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')\n",
    "\n",
    "# Preprocess test set\n",
    "# sequence encode\n",
    "encoded_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# pad sequences\n",
    "X_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.648317400Z"
    }
   },
   "id": "a5fe0d6099fdb64e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.int).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.int).to(device)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.649319Z"
    }
   },
   "id": "1276fa4266d45fef",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pretrained Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5187519f8711461"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load pretrained glove twitter embeddings\n",
    "import gensim\n",
    "\n",
    "# Keep commented out since this load takes a while. Save it to disk for quicker use\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format(r'C:\\Users\\connor\\PycharmProjects\\rice\\comp_647\\assignment_2\\pretrained_embeddings\\glove.twitter.27B.200d_wv.txt')\n",
    "# model.save(r'C:\\Users\\connor\\PycharmProjects\\rice\\comp_647\\assignment_2\\pretrained_embeddings\\glove.twitter.27B.200d_usable_weights')\n",
    "model = gensim.models.KeyedVectors.load(\n",
    "    r'C:\\Users\\connor\\PycharmProjects\\rice\\comp_647\\assignment_2\\pretrained_embeddings\\glove.twitter.27B.200d_usable_weights')\n",
    "weights = torch.FloatTensor(model.vectors)\n",
    "embedding = nn.Embedding.from_pretrained(weights, freeze=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.650316600Z"
    }
   },
   "id": "13cc9cf68264b7ec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Confirm embeddings work, get embeddings for index 1\n",
    "input = torch.LongTensor([1])\n",
    "embedding(input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.650316600Z"
    }
   },
   "id": "2bb0a410c1852bc0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Twitter-roBERTa-base"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f54a63bc5480c936"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "twitter_roberta = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.651316100Z"
    }
   },
   "id": "1eda704326759473",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "twitter_roberta"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.652316Z"
    }
   },
   "id": "65ab82621e9c3ca4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# You must batch twitter_roberta or it will run out of memory\n",
    "class TWITTERROBERTA(nn.Module):\n",
    "    def __init__(self, pretrained_model=twitter_roberta, requires_grad=False, classes=5):\n",
    "        super().__init__()\n",
    "        self.twitter_roberta = pretrained_model\n",
    "        \n",
    "        self.twitter_roberta.transform_input=False\n",
    "        self.twitter_roberta.classifier.out_proj = nn.Linear(768, classes)\n",
    "    \n",
    "        # freeze the last layer if false\n",
    "        if not requires_grad:\n",
    "            self._freeze_param()\n",
    "    \n",
    "    def _freeze_param(self):\n",
    "        for k,v in self.named_parameters():\n",
    "            if k.startswith(\"out_proj\"):\n",
    "                v.requires_grad = True\n",
    "                                   \n",
    "    def forward(self, x):\n",
    "        return self.twitter_roberta(x).logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.653317400Z"
    }
   },
   "id": "c1bf2acf31061e70",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Need to create dataloader object for twitter-roBERTa since it is too big. \n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.654316700Z"
    }
   },
   "id": "96152349a4f19fe9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## confirm dataloader is functioning properly\n",
    "# for train, label in train_dataloader:\n",
    "#     print(train)\n",
    "#     print()\n",
    "#     print(label)\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.655317500Z"
    }
   },
   "id": "698d803bf2c9fa06",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer Encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "811acdd506cce160"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# not working correctly when batched\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, heads, num_linear_layers, num_classes=5):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # create embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=max_length)\n",
    "        # Initialize the encoder \n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=max_length, nhead=heads), num_layers=num_linear_layers)\n",
    "        # Define the fully connected layer\n",
    "        self.linear = nn.Linear(max_length, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the transformer encoder \n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1) \n",
    "        return self.linear(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.656317100Z"
    }
   },
   "id": "e868ae6c08007ea6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53f447b4ea5c25ca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, cnn_num_classes=5, dropout_rate=0, batch_norm=False, num_linear_layers=3, num_neurons=16, num_cnn_layers=3, num_filters=32, activation_function=nn.ReLU(), pretrained_embedding=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # feature extraction\n",
    "        if pretrained_embedding:\n",
    "            self.embedding = embedding\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=max_length)\n",
    "        self.cnn = nn.Conv1d(in_channels=max_length, out_channels=num_filters, kernel_size=3, padding='same')\n",
    "        self.flatten = nn.Flatten()\n",
    "        # first and last linear layer and other one use layers\n",
    "        self.linear = nn.Linear(in_features=max_length*num_filters, out_features=num_neurons)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.output = nn.Linear(in_features=num_neurons, out_features=cnn_num_classes)\n",
    "        self.activation_function = activation_function\n",
    "        self.batch_normalization = nn.BatchNorm1d(num_features=num_neurons)\n",
    "        \n",
    "        # layer lists\n",
    "        self.dropout_list = nn.ModuleList([nn.Dropout(dropout_rate) for dummy in range(num_linear_layers-1)])\n",
    "        self.batch_normalization_list = nn.ModuleList([nn.BatchNorm1d(num_features=num_neurons) for dummy in range(num_linear_layers-1)]) #batch norm doesn't make sense for unbatched data which is what I have\n",
    "        self.hidden_layers_list = nn.ModuleList([nn.Linear(num_neurons, num_neurons) for dummy in range(num_linear_layers-1)])\n",
    "        self.hidden_cnn_layers_list = nn.ModuleList([nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=3, padding='same') for dummy in range(num_cnn_layers-1)])\n",
    "        \n",
    "        # parameters used in forward function\n",
    "        self.num_linear_layers = num_linear_layers\n",
    "        self.num_cnn_layers = num_cnn_layers\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.cnn(x)\n",
    "        for layer in range(self.num_cnn_layers-1):\n",
    "            x = self.hidden_cnn_layers_list[layer](x)\n",
    "        x = self.flatten(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.dropout(self.activation_function(self.batch_normalization(self.linear(x))))\n",
    "        else:\n",
    "            x = self.dropout(self.activation_function(self.linear(x)))\n",
    "        for layer in range(self.num_linear_layers-1):\n",
    "            if self.batch_norm:\n",
    "                x = self.dropout_list[layer](self.activation_function(self.batch_normalization_list[layer](self.hidden_layers_list[layer](x))))\n",
    "            else:\n",
    "                x = self.dropout_list[layer](self.activation_function(self.hidden_layers_list[layer](x)))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.657317300Z"
    }
   },
   "id": "5be82ad2cdcab5f1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_twitter_roberta(model, criterion, optimizer, num_epochs, model_name, train_dataloader=train_dataloader, test_dataloader=test_dataloader, early_stopping=True, device=device):\n",
    "    epoch_train_loss_list = []\n",
    "    epoch_train_accuracy_list = []\n",
    "    epoch_valid_loss_list = []\n",
    "    epoch_valid_accuracy_list = []\n",
    "    early_stopping_counter = 0\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for train_x_data, train_y_data in train_dataloader:\n",
    "            train_x_data.to(device)\n",
    "            train_y_data.to(device)\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_x_data)\n",
    "            loss_train = criterion(outputs, train_y_data)\n",
    "            \n",
    "            # Take softmax of the output -> get the argmax to convert it to a single number -> one hot encode to get it to the same format as the label. \n",
    "            softmax_outputs = torch.nn.functional.one_hot(torch.argmax(nn.Softmax(dim=0)(outputs), dim=1)).to(torch.float)\n",
    "            total_right = 0\n",
    "            \n",
    "            # update since now using batch sizes that aren't equal to the full data length\n",
    "            for row in range(len(train_y_data)):\n",
    "                # need to use .equal since torchmetrics.accuracy will evaluate each value in each row. Ex.) pred = [0,0,1,0,0,0], actual = [1,0,0,0,0,0] as 66% correct. Expected behavior is this would be 0%\n",
    "                if torch.equal(softmax_outputs[row], train_y_data[row]):\n",
    "                    total_right += 1       \n",
    "            epoch_accuracy_train = total_right / len(train_y_data)\n",
    "        \n",
    "            epoch_train_loss_list.append(loss_train.item())\n",
    "            epoch_train_accuracy_list.append(epoch_accuracy_train)\n",
    "        \n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for valid_x_data, valid_y_data in test_dataloader:\n",
    "                valid_x_data.to(device)\n",
    "                valid_y_data.to(device)\n",
    "                \n",
    "                outputs = model(valid_x_data)\n",
    "                loss_valid = criterion(outputs, valid_y_data)\n",
    "                best_accuracy = -np.inf\n",
    "                \n",
    "                softmax_outputs = torch.nn.functional.one_hot(torch.argmax(nn.Softmax(dim=0)(outputs), dim=1)).to(torch.float)\n",
    "                total_right = 0\n",
    "                for row in range(len(valid_y_data)):\n",
    "                    # need to use .equal since torchmetrics.accuracy will evaluate each value in each row. Ex.) pred = [0,0,1,0,0,0], actual = [1,0,0,0,0,0] as 66% correct. Expected behavior is this would be 0%\n",
    "                    if torch.equal(softmax_outputs[row], valid_y_data[row]):\n",
    "                        total_right += 1       \n",
    "                epoch_accuracy_valid = total_right / len(valid_y_data)\n",
    "        \n",
    "                epoch_valid_loss_list.append(loss_valid.item())\n",
    "                epoch_valid_accuracy_list.append(epoch_accuracy_valid)\n",
    "\n",
    "        if loss_valid.item() < best_loss:\n",
    "            early_stopping_counter = 0\n",
    "            best_loss = loss_valid.item()\n",
    "            torch.save(model.state_dict(), fr'C:\\Users\\connor\\PycharmProjects\\comp_647_final\\model_weights\\{model_name}')\n",
    "\n",
    "        # if epoch_accuracy_valid >= best_accuracy:\n",
    "        #     best_accuracy = epoch_accuracy_valid\n",
    "        #     torch.save(model.state_dict(), fr'C:\\Users\\connor\\PycharmProjects\\comp_647_final\\model_weights\\{model_name}_accuracy')\n",
    "\n",
    "        # Stop training if after early_stopping_patience epochs, validation loss does not go down\n",
    "        print('Epoch:', epoch, 'Best Valid Loss:', best_loss, 'Current Valid loss:', loss_valid.item(), 'Current Train loss:', loss_train.item(), 'Stop Counter:', early_stopping_counter)\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping and early_stopping_counter >= 10:\n",
    "            print(\"Early Stopping Triggered\")\n",
    "            break\n",
    "        \n",
    "    return epoch_train_loss_list, epoch_valid_loss_list, epoch_train_accuracy_list, epoch_valid_accuracy_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.658316900Z"
    }
   },
   "id": "23e955da15afd3a9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_x_data, train_y_data, valid_x_data, valid_y_data, num_epochs, model_name, early_stopping=True, device=device):\n",
    "    epoch_train_loss_list = []\n",
    "    epoch_train_accuracy_list = []\n",
    "    epoch_valid_loss_list = []\n",
    "    epoch_valid_accuracy_list = []\n",
    "    early_stopping_counter = 0\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    train_x_data.to(device)\n",
    "    train_y_data.to(device)\n",
    "    valid_x_data.to(device)\n",
    "    valid_y_data.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_x_data)\n",
    "        loss_train = criterion(outputs, train_y_data)\n",
    "        \n",
    "        # Take softmax of the output -> get the argmax to convert it to a single number -> one hot encode to get it to the same format as the label. \n",
    "        softmax_outputs = torch.nn.functional.one_hot(torch.argmax(nn.Softmax(dim=0)(outputs), dim=1)).to(torch.float)\n",
    "        total_right = 0\n",
    "        for row in range(len(train_y_data)):\n",
    "            # need to use .equal since torchmetrics.accuracy will evaluate each value in each row. Ex.) pred = [0,0,1,0,0,0], actual = [1,0,0,0,0,0] as 66% correct. Expected behavior is this would be 0%\n",
    "            if torch.equal(softmax_outputs[row], train_y_data[row]):\n",
    "                total_right += 1       \n",
    "        epoch_accuracy_train = total_right / len(train_y_data)\n",
    "        \n",
    "        epoch_train_loss_list.append(loss_train.item())\n",
    "        epoch_train_accuracy_list.append(epoch_accuracy_train)\n",
    "        \n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(valid_x_data)\n",
    "            loss_valid = criterion(outputs, valid_y_data)\n",
    "            best_accuracy = -np.inf\n",
    "            \n",
    "            softmax_outputs = torch.nn.functional.one_hot(torch.argmax(nn.Softmax(dim=0)(outputs), dim=1)).to(torch.float)\n",
    "            total_right = 0\n",
    "            for row in range(len(valid_y_data)):\n",
    "                # need to use .equal since torchmetrics.accuracy will evaluate each value in each row. Ex.) pred = [0,0,1,0,0,0], actual = [1,0,0,0,0,0] as 66% correct. Expected behavior is this would be 0%\n",
    "                if torch.equal(softmax_outputs[row], valid_y_data[row]):\n",
    "                    total_right += 1       \n",
    "            epoch_accuracy_valid = total_right / len(valid_y_data)\n",
    "    \n",
    "            epoch_valid_loss_list.append(loss_valid.item())\n",
    "            epoch_valid_accuracy_list.append(epoch_accuracy_valid)\n",
    "\n",
    "        if loss_valid.item() < best_loss:\n",
    "            early_stopping_counter = 0\n",
    "            best_loss = loss_valid.item()\n",
    "            torch.save(model.state_dict(), fr'C:\\Users\\connor\\PycharmProjects\\comp_647_final\\model_weights\\{model_name}')\n",
    "\n",
    "        # if epoch_accuracy_valid >= best_accuracy:\n",
    "        #     best_accuracy = epoch_accuracy_valid\n",
    "        #     torch.save(model.state_dict(), fr'C:\\Users\\connor\\PycharmProjects\\comp_647_final\\model_weights\\{model_name}_accuracy')\n",
    "\n",
    "        # Stop training if after early_stopping_patience epochs, validation loss does not go down\n",
    "        print('Epoch:', epoch, 'Best Valid Loss:', best_loss, 'Current Valid loss:', loss_valid.item(), 'Current Train loss:', loss_train.item(), 'Stop Counter:', early_stopping_counter)\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping and early_stopping_counter >= 10:\n",
    "            print(\"Early Stopping Triggered\")\n",
    "            break\n",
    "        \n",
    "    return epoch_train_loss_list, epoch_valid_loss_list, epoch_train_accuracy_list, epoch_valid_accuracy_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.658316900Z"
    }
   },
   "id": "605edc1d38f6ac81",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_accuracy(model, X_test_data, y_test_data):\n",
    "    # epoch_accuracy_valid = Accuracy(task=\"multiclass\", num_classes=6).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_data)\n",
    "        # Take softmax of the output -> get the argmax to convert it to a single number -> one hot encode to get it to the same format as the label. \n",
    "        softmax_outputs = torch.nn.functional.one_hot(torch.argmax(nn.Softmax(dim=0)(outputs), dim=1)).to(torch.float)\n",
    "        #softmax_outputs = torch.nn.functional.one_hot(torch.argmax(nn.Softmax(dim=-1)(outputs), dim=1)).to(torch.float)\n",
    "        \n",
    "        total_right = 0\n",
    "        for row in range(len(y_test_data)):\n",
    "            # need to use .equal since torchmetrics.accuracy will evaluate each value in each row. Ex.) pred = [0,0,1,0,0,0], actual = [1,0,0,0,0,0] as 66% correct. Expected behavior is this would be 0%\n",
    "            if torch.equal(softmax_outputs[row], y_test_data[row]):\n",
    "                total_right += 1\n",
    "                \n",
    "        accuracy = total_right / len(y_test_data)\n",
    "\n",
    "    return accuracy, softmax_outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.659317500Z"
    }
   },
   "id": "972cc4b831c0404a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(train_loss_list, valid_loss_list, train_accuracy_list, valid_accuracy_list, title):\n",
    "    \"\"\"\n",
    "    train_loss_list: list containing the loss per epoch of the training data during training\n",
    "    valid_loss_list: list containing the loss of the valid dataset using the trained dataset at the end of each epoch\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=2)\n",
    "    fig.suptitle(title,x=.55, y=1.75, fontsize=20)\n",
    "    \n",
    "    axes[0].set_title('Training and Validation Loss')    \n",
    "    axes[0].plot(train_loss_list, color='b', label='Training Loss')\n",
    "    axes[0].plot(valid_loss_list, color='orange', label='Validation Loss')\n",
    "    axes[0].legend(['Train', 'Valid'])\n",
    "    axes[0].set_xlabel('Num Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    \n",
    "    axes[1].set_title('Training and Validation Accuracy')    \n",
    "    axes[1].plot(train_accuracy_list, color='b', label='Training Accuracy')\n",
    "    axes[1].plot(valid_accuracy_list, color='orange', label='Validation Accuracy')\n",
    "    axes[1].legend(['Train', 'Valid'])\n",
    "    axes[1].set_xlabel('Num Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=1.5)\n",
    "    \n",
    "    return fig, axes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.660317400Z"
    }
   },
   "id": "3a27cab049703069",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def trainable_parameters_count(model):\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.660317400Z"
    }
   },
   "id": "c7318d6316290076",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Random Grid Search Func\n",
    "def random_grid_search(architecture_type, lr_list, activation_function_list, neuron_num_list, batch_norm_list, dropout_rate_list, num_linear_layers, num_cnn_layers, num_epochs, early_stop_num, num_classes=5, num_combos=5, seed_value=None, pretrained_embeddings=False):\n",
    "    # set seed for reproducibility\n",
    "    random.seed(seed_value)\n",
    "    \n",
    "    best_model_loss = np.inf\n",
    "    best_rand_grid_model = None\n",
    "    # randomly choose combo\n",
    "    for dummy in range(num_combos):\n",
    "        lr = random.choice(lr_list)\n",
    "        activation = random.choice(activation_function_list)\n",
    "        linear_layers = random.choice(num_linear_layers_list)\n",
    "        cnn_layers = random.choice(num_cnn_layers_list)\n",
    "        neuron_num = random.choice(neuron_num_list)\n",
    "        batch_norm_t_f = random.choice(batch_norm_list)\n",
    "        dropout = random.choice(dropout_rate_list)\n",
    "        network_name = str(lr) + '_' + str(activation) + '_' + str(neuron_num) + '_' + str(batch_norm_t_f) + '_' + str(dropout) + '_' + str(linear_layers) + '_' + str(cnn_layers) + '_' +str(architecture_type) + '_' +str(pretrained_embeddings)\n",
    "        \n",
    "        if architecture_type == 'CNN':\n",
    "            grid = CNN(cnn_num_classes=num_classes, activation_function=activation, num_neurons=neuron_num, num_linear_layers=linear_layers, num_cnn_layers=cnn_layers, batch_norm=batch_norm_t_f, dropout_rate=dropout, pretrained_embedding=pretrained_embeddings)\n",
    "        # REPLACE WITH SECOND MODEL TYPE\n",
    "        elif architecture_type == 'Transformer':\n",
    "            grid = CNN(cnn_num_classes=num_classes, activation_function=activation, num_neurons=neuron_num, num_linear_layers=linear_layers, num_cnn_layers=cnn_layers, batch_norm=batch_norm_t_f, dropout_rate=dropout)\n",
    "        grid.to(device)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(grid.parameters(), lr=lr)\n",
    "        print('Network Name:', network_name, 'Trainable Parameters:', trainable_parameters_count(grid))\n",
    "        train_loss, valid_loss, train_acc, valid_acc = train_model(model=grid, criterion=loss, optimizer=optimizer, train_x_data=X_train, valid_x_data=X_test, train_y_data=y_train, valid_y_data=y_test, num_epochs=num_epochs, early_stopping=False, model_name=network_name, device=device)\n",
    "        \n",
    "        # plot training and validation loss as well as accuracy for both\n",
    "        plot_loss_and_accuracy(train_loss, valid_loss, train_acc, valid_acc, title=network_name)\n",
    "        # keep track of which model had the lowest loss of all models. This model is saved as network_name in the model_weights directory\n",
    "        min_valid_loss = np.inf\n",
    "        for val in valid_loss:\n",
    "            if val < min_valid_loss:\n",
    "                min_valid_loss = val\n",
    "                \n",
    "        if min_valid_loss < best_model_loss:\n",
    "            best_model_loss = min_valid_loss\n",
    "            best_rand_grid_model = network_name\n",
    "            \n",
    "    return best_rand_grid_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.661317500Z"
    }
   },
   "id": "93b73e5d72099eba",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0fea29ac54c88ca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# parameter tuning\n",
    "num_classes = 5\n",
    "num_epochs = 1000\n",
    "early_stop_num = 100\n",
    "num_linear_layers_list = [3,4,5,8]\n",
    "num_cnn_layers_list = [3,4,5,8]\n",
    "lr_list = [.0001, .00001, .000001, .0000001]\n",
    "activation_function_list = [nn.ReLU(), nn.LeakyReLU(), nn.Softmax(dim=0)]\n",
    "dropout_rate_list = [.1, .2, .5, .75]\n",
    "neuron_num_list = [16, 32, 64, 128, 256, 512]\n",
    "batch_norm_list = [True, False]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.661317500Z"
    }
   },
   "id": "30542ebe61d34f17",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# best_transformer_model = random_grid_search(\n",
    "#     architecture_type='Transformer', \n",
    "#     lr_list=lr_list, \n",
    "#     activation_function_list=activation_function_list, \n",
    "#     neuron_num_list=neuron_num_list, \n",
    "#     batch_norm_list=batch_norm_list, \n",
    "#     dropout_rate_list=dropout_rate_list, \n",
    "#     num_epochs=num_epochs, \n",
    "#     early_stop_num=early_stop_num, \n",
    "#     num_cnn_layers=num_linear_layers_list, \n",
    "#     num_linear_layers=num_cnn_layers_list,\n",
    "#     num_combos=20, \n",
    "#     seed_value=16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.662317Z"
    }
   },
   "id": "472324ff7fb36ede",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "transformer_model = TransformerEncoder(heads=8, num_linear_layers=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.662317Z"
    }
   },
   "id": "2f18d9a8a5a6174e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# best_cnn_model = random_grid_search(\n",
    "#     architecture_type='CNN', \n",
    "#     lr_list=lr_list, \n",
    "#     activation_function_list=activation_function_list, \n",
    "#     neuron_num_list=neuron_num_list, \n",
    "#     batch_norm_list=batch_norm_list, \n",
    "#     dropout_rate_list=dropout_rate_list, \n",
    "#     pretrained_embeddings=True,\n",
    "#     num_epochs=num_epochs, \n",
    "#     early_stop_num=early_stop_num, \n",
    "#     num_cnn_layers=num_linear_layers_list, \n",
    "#     num_linear_layers=num_cnn_layers_list,\n",
    "#     num_combos=20, \n",
    "#     seed_value=16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.663317400Z"
    }
   },
   "id": "5e4270887bff524a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_model = CNN(cnn_num_classes=5, activation_function=nn.LeakyReLU(), num_neurons=256, num_linear_layers=3, num_cnn_layers=3, batch_norm=False, dropout_rate=.2)\n",
    "best_model.to('cpu')\n",
    "best_model.load_state_dict(torch.load(r'C:\\Users\\connor\\PycharmProjects\\comp_647_final\\model_weights\\0.0001_LeakyReLU(negative_slope=0.01)_256_False_0.2_3_3_1', weights_only=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.663317400Z"
    }
   },
   "id": "6ef0f69099fac4e5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# twitter_roberta test\n",
    "t_roberta = TWITTERROBERTA()\n",
    "t_roberta.to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(t_roberta.parameters(), lr=.0001)\n",
    "epoch_train_loss_list, epoch_valid_loss_list, epoch_train_accuracy_list, epoch_valid_accuracy_list = train_twitter_roberta(t_roberta, loss, optimizer, 10, 'ROBERTA TEST')\n",
    "plot_loss_and_accuracy(epoch_train_loss_list, epoch_valid_loss_list, epoch_train_accuracy_list, epoch_valid_accuracy_list, title='ROBERTA TEST')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.664317Z"
    }
   },
   "id": "7ba8b54d4e957d00",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_model = TWITTERROBERTA()\n",
    "best_model.to('cpu')\n",
    "best_model.load_state_dict(torch.load(r'C:\\Users\\connor\\PycharmProjects\\comp_647_final\\model_weights\\ROBERTA TEST', weights_only=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.665316800Z"
    }
   },
   "id": "40866f1f716febd7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.665316800Z"
    }
   },
   "id": "7bb2b24031bfcd08"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "193cc3b64becc09e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "encoded_test = tokenizer.texts_to_sequences(test.text)\n",
    "\n",
    "# pad sequences\n",
    "test = pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "test = torch.tensor(test, dtype=torch.int).to('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.666317300Z"
    }
   },
   "id": "a4dcfd38ae6647a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = best_model(torch.tensor(test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.666317300Z"
    }
   },
   "id": "f343eaa6de3d9ce8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "softmax_outputs = torch.nn.functional.one_hot(torch.argmax(nn.Softmax(dim=0)(outputs), dim=1)).to(torch.float)\n",
    "softmax_outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.667317300Z"
    }
   },
   "id": "d16b161749d76537",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predictions = pd.from_dummies(pd.DataFrame(softmax_outputs.numpy()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.667317300Z"
    }
   },
   "id": "6929ba060c904564",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predictions.columns = ['sentiment']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.668320900Z"
    }
   },
   "id": "cd4875101ed7b338",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predictions['id'] = predictions.index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.668320900Z"
    }
   },
   "id": "c8ea9e9cbc472a4c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predictions = predictions[['id', 'sentiment']]\n",
    "predictions.id = [x + 1 for x in predictions.id]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.669322Z"
    }
   },
   "id": "a84cf9466e5d1c77",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predictions.reset_index(drop=True, inplace=True)\n",
    "predictions.to_csv(r'C:\\Users\\connor\\PycharmProjects\\comp_647_final\\predictions.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.669322Z"
    }
   },
   "id": "3d17b739086c9346",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-17T14:01:56.670322100Z"
    }
   },
   "id": "70e1bc7e010b6656",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
